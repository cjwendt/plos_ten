% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}


\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}



\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{Stats,Math,ERHS}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Ten simple rules for finding and selecting R packages} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Caroline J. Wendt\textsuperscript{\getAff{Stats}, \getAff{Math}},
G. Brooke Anderson\textsuperscript{\getAff{ERHS}}\textsuperscript{*}\\
\bigskip
\textbf{\getAff{Stats}}Department of Statistics, Colorado State University, Fort Collins,
Colorado, United States of America\\
\textbf{\getAff{Math}}Department of Mathematics, Colorado State University, Fort Collins,
Colorado, United States of America\\
\textbf{\getAff{ERHS}}Department of Environmental \& Radiological Health Sciences, Colorado
State University, Fort Collins, Colorado, United States of America\\
\bigskip
* Corresponding author: Brooke.Anderson@colostate.edu\\
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
R is an increasingly preferred software environment for data analytics
and statistical computing among scientists and practitioners. Packages
markedly extend R's utility and ameliorate inefficient solutions. We
outline ten simple rules for finding relevant packages and determining
which package is best for your desired use.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.
\section*{Author summary}
Write the author summary here. Do we want to include and author summary?

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\emph{Text based on plos sample manuscript, see
\url{http://journals.plos.org/ploscompbiol/s/latex}}

\hypertarget{disclaimer}{%
\section{Disclaimer?}\label{disclaimer}}

Do we need to include a disclaimer in the margin like the one from
{[}1{]} that states: ``\textbf{Competing Interests}: The authors have no
affiliation with GitHub, nor with any other commercial entity mentioned
in this article. The views described here reflect their own views
without input from any third party organization.''

\begin{itemize}
\tightlist
\item
  RStudio
\item
  ROpenSci
\item
  GitHub
\end{itemize}

I am an editor at ROpenSci, so we could mention that in the disclaimer.
I do not have any financial interests from that position (it's
volunteer, as are many journal editing positions in academics).

\hypertarget{funding-acknowledgment}{%
\section{Funding acknowledgment}\label{funding-acknowledgment}}

{[}Funding acknowledgement---Add in grant number for R25 and
acknowledgment of Honors program if appropriate{]}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Computational reproducibility is surfacing as a central axiom in
academia, as researchers identify the need for transparency {[}2,3{]}.
The Methods section of scholarly work is not sufficient to convey the
complex data analysis methods and powerful tools of our time---data and
code must be shared. Some variability in scientific outcomes can be
attributed to differences in workflow and, today, the scientific
community deems the absence of automation to be irresponsible {[}4{]}.
Many traditional approaches that produce such inconsistencies also tend
to be at odds with productivity and collaboration. While standards for
disseminating and communicating computational science are evolving with
digitization, responsible researchers are adopting best practices so
others can more easily interpret, validate, and extend their published
knowledge {[}5{]}. These researchers rely on accessible and robust tools
to ensure their computations are reproducible.

As the R ecosystem---in which the life of modern data analysis
thrives---rapidly evolves alongside the burgeoning R community, R is
exhibiting sustained growth when compared to similar languages,
particularly in academia, healthcare, and government {[}6{]}. The open
source R language has become a dominant quantitative programming
environment in academic data analysis, enabling researchers to share
workflows and re-execute scripts within and across subsets of the
scientific community {[}4{]}. R is increasingly popular in computational
biology and bioinformatics, two of many disciplines generating
extensive, heterogenous, and complex data wanting for heavy-duty data
analysis tools that (ideally) support reproducibility {[}7,8{]}.

R was developed by statisticians and is collaboratively maintained by an
international group of core contributors {[}9{]}. Unlike several popular
proprietary languages such as MATLAB or SAS, R is freely available, open
source, and easily extensible software; the user can access, alter,
extend, and share code in various applications. Accordingly, a vibrant
community of R users has emerged, many of whom develop R packages to
complement and extend the functionality of base R. An R package is the
fundamental unit of shareable code. Contributed packages comprise the
bulk of enhancements made to the R environment {[}10{]}. In fact, much
of what we know about statistical methods and algorithms is wrapped up
in R packages---written and documented in various ways by R users. There
are numerous analogies in computing between programming and culinary
arts: recipe structures, coding cookbooks, and the like. To
conceptualize packages, imagine you are the chef, base R is the kitchen,
and packages are the special gadgets which allow you to cook and bake
new recipes. ``R package'' \emph{technically} refers to a collection of
R functions in a certain structure. The few tools that come with your
kitchen (e.g., \texttt{stats}, \texttt{utils}) are indeed R packages,
whereas ``R extension'' describes the tools you add to your kitchen
{[}11{]}. It's important to recognize that ``package'' is not synonymous
with ``extension''; however, since we don't want to keep distinguishing
a kitchen sink from a mixer, we use ``package'' in the colloquial sense.
In essence, R packages are coding delectables that enable the user to
perform practical tasks and solve problems with interesting techniques.

Are there R packages for wrangling and cleaning data frames, designing
interactive applications for data visualization, or performing
dimensionality reduction? Yes! How do you \emph{find} an R package to
help you train regression and classification models, assess the beta
diversity of a population, or analyze gene expression microarray data?
This answer is not as simple; there are tens of thousands of R packages.
As a natural consequence of the open source nature of R, there is
considerable variation in the quality of R packages and nontrivial
differences among those that provide similar tools.

Packages are essential to venturing beyond base R and, thus, quickly
become an integral aspect of advancing your R skills. Those who have
used R packages may know that, although leveraging existing tools can be
advantageous, the initial challenge of finding a suitable package for a
given task can obstruct potential benefits. The advanced R user---having
developed an intuition for their workflow---may be relatively confident
when searching for and selecting packages. By contrast, new R users who
are unfamiliar with the structure and syntax of the language may be
hindered by the process of finding packages because they do not know
where to search, what to look for, or how to sift through options. An
obstacle that characterizes learning R at the outset is the struggle to
(1) find a package to accomplish a particular task or solve a problem of
interest and (2) choose the best package to perform that task. Even so,
some obscure and complicated recipes make it difficult for an
experienced chef to select the best tools.

In both coding and life, we endeavor to make choices that optimize
outcomes. Just as one may go about shopping for shoes, deciding which
graduate program to pursue, or conducting a literature review, there is
a science behind selection. We inform our decisions by assessing,
comparing, and filtering options based on indicators of quality such as
utility, association, and reputation. Likewise, choosing an R package
requires attending to similar details. We outline ten simple rules for
finding and selecting R packages, so that you will spend less time
searching for the right tools and more time coding delightful recipes.

\hypertarget{rule-1-consider-your-purpose}{%
\section{Rule 1: Consider your
purpose}\label{rule-1-consider-your-purpose}}

Usually, there are several ways to accomplish a task while programming,
albeit some more elegant and efficient than others. Before looking for a
package to use for a task, determine whether you need one. Consider your
purpose by first identifying your goal then defining the scope and tasks
to achieve it. For some tasks and workflows, coding your own recipe with
existing tools is practical, while others benefit from new tools.

If the scope of your task is simple or reasonable, given your knowledge
and skills, using an R package may not be appropriate. There can be
advantages of coding in base R to complete your task or solve your
problem. First, when you code from scratch, you know precisely what you
are running; thus, your script may be easier to decipher and maintain
over time. Conversely, packages require you to rely on shared code with
features or underlying processes of which you may not be aware. Second,
although base R is relatively stable and slow to change, many R packages
evolve rapidly; unbeknown changes made to packages can cause defects in
portions of your code affected by these changes.

Packages are favorable in the same sense as kitchen tools: when a task
has a broad or complex scope beyond what you can (or desire to) attempt
from scratch. While there are ways to cook up an algorithm using
\texttt{for} loops and conditionals in base R, a relevant package may
accomplish your goal with less code and fewer bugs. The more reasonable
it is for a given task to be abstracted away from its context, the more
likely someone has generalized its themes, developed efficient
algorithms, and intuitively organized them to share with other R users.
Extensive tasks justify sophisticated frameworks with several functions
that form a cohesive package or even a suite of related packages. Data
manipulation is one such common task that has been streamlined by
packages such as \texttt{dplyr} and \texttt{tidyr} {[}12,13{]}, both
part of the \texttt{tidyverse} suite (see Table 1) {[}14{]}.
Nevertheless, don't be discouraged if you have a task that seems too
unusual for a package. There are indeed packages for seemingly singular
tasks, which you may favor over coding from scratch. Surprisingly, you
can use an R package to access the Twitter API, send emails from R, and
there is even a package for converting English letters to numbers as on
a telephone keypad {[}15--17{]}.

If you decide you need a package, next ask yourself: Which
functionalities in base R are restrictive in context of your task? Which
new functionalities would expand what you can do? If you identify
limitations of your current toolbox before searching for new tools, you
will be primed to recognize what you do and don't need. List
domain-specific keywords that describe what you are trying to do and how
you could do it, so you narrow your search. Identify the type of inputs
you have and envision working with them; contemplate the desired outputs
and corresponding format. Suppose you are using Bioconductor packages in
analyses and have outputs you want to visualize; you must consider that
the inputs may be of a certain class---namely, S4 objects---which impose
restrictions when creating graphics {[}18{]}. When you look for a
package to visualize the data, you will want to choose one that
addresses such restrictions. If, however, you don't quite know (or have
a difficult time articulating) what you need, don't hesitate to search
the internet for examples that capture your purpose. Sometimes, for
instance, it's helpful to search Google Images for plots and figures to
identify the terminology that describes what you are trying to do, how
people approach similar tasks, and potential challenges that may arise.

\hypertarget{rule-2-find-and-collect-options}{%
\section{Rule 2: Find and collect
options}\label{rule-2-find-and-collect-options}}

Tips and leads for finding R packages exist in a variety of places
online, in print, and elsewhere. If you've searched for packages a lot,
you likely have a shortlist of tried and true starting points---a perk
of being a long-time R user. On the other hand, new R users who haven't
developed an internal compass to navigate the initial ins and outs of
finding a package, don't know where to start looking for a package to
suit their task. Here are a few directions to head in: You can discover
new packages any time you learn R-related topics, browse the internet,
or connect with the international community of R users.

\textbf{Learn}

When learning how to program in R, you are typically introduced to some
of the most common packages, which tend to have more general purposes
(\textbf{Table 1}). In addition, reputable online tutorials, courses,
and books are helpful resources for acquiring knowledge about packages
that are versatile and reliable---many of which are short, accessible,
and either affordable or offered at
\href{https://committedtotape.shinyapps.io/freeR/}{no cost} to the
learner. We recommend online R programming courses such as those through
\href{https://www.coursera.org/learn/r-programming}{Coursera} and
\href{https://www.codecademy.com/learn/learn-r}{Codeacademy} for
interactive learning and R book series including the
\href{https://rstudio.com/resources/books/}{RStudio books} and Springer
titles for further reading.

\textbf{Browse}

The solution-seeking tactics we employ for many tasks nowadays may lead
you to think that finding R packages relies heavily on internet search
queries. Indeed, search engines such as Google return ample pages
related to anything \texttt{"\ldots{}in\ R"}. Keep in mind, however,
this approach sometimes leads to frustration and confusion when
attempting to find a package tailored to your purpose (see Rule 1). For
a more directed approach, you can search the package lists on
repositories including the Comprehensive R Archive Network (CRAN),
Bioconductor, and rOpenSci or browse packages available on code-sharing
websites, with language filters for finding repositories that heavily
rely on R code, like GitHub and GitLab---all of which will be further
discussed in Rule 3.

That said, in many cases, you may find it more helpful to start from a
curated list of R packages on a particular topic. There are several
available. First, \href{https://cran.r-project.org/web/views/}{CRAN Task
Views} are concentrated topics from certain disciplines and
methodologies related to statistical computing that categorize R
packages by the tasks they perform (e.g., Econometrics, Genetics,
Optimization, Spatial). In the HTML version, you can browse alphabetized
subcategories within each Task View and read concise descriptions to
find tools with specific functions. Alternatively, you can access Task
Views directly from the R console with \texttt{ctv::CRAN.views()}, which
is compact notation of the form \texttt{package::function()} {[}19{]}.
To date, there are 41 Task Views that collectively contain thousands of
packages which are curated and regularly tested. Moreover, CRAN Task
Views provide tools that enable you to automatically install all
packages within a targeted area of interest. Ultimately, by providing
task-based organization, easy simultaneous installation of related
packages, meta-information, ensured maintenance, and quality control,
CRAN Task Views address several major user-end issues that have arisen
due to the sheer quantity of available packages {[}20{]}.

The computational biology and bioinformatics analog of CRAN Task Views,
\href{https://www.bioconductor.org/packages/release/BiocViews.html\#___Software}{biocViews},
is a list of packages, available through the Bioconductor website, that
provide tools for analyzing biological data. Your keywords from Rule 1
will come in handy here; for instance, you can search ``RNA-seq'' to
find all packages related to high-throughput sequencing methods for
analyzing RNA populations. You can also browse the biocViews tree menu
to narrow your search based on which general and specific categories
describe your task. The list includes each package's name linked to
information and documentation, its maintainer, a short description, and
its rank relative to other Bioconductor packages.

Other curated collections and topically-linked lists of packages are
also available.
\href{http://dirk.eddelbuettel.com/cranberries/index.html}{CRANberries},
for example, is a hub of information about new, updated, and removed
packages from CRAN. Another place to find well-maintained tools is
\href{https://ropensci.org/packages/}{rOpenSci packages}. These
filterable and searchable peer-reviewed (see Rule 6) R packages are
organized by name, maintainer, description, and status markers based on
activity, association, and review.

\textbf{Embrace the community}

An inclusive and collaborative community is an overlooked, yet integral,
aspect of a software's success {[}21{]}. A defining feature of R is the
enthusiasm of its users and developers alike. The R community has a
widespread internet presence across various platforms; however, members
are markedly active on Twitter, a place where R users seek help, share
ideas, and stay informed on \texttt{\#rstats} happenings, including
releases of new packages {[}22{]}. R-related blogs serve as another
informal, up-to-date, and more detailed avenue for communicating and
promoting R-related information (\textbf{Table ?}). For example, Joseph
Rickert, Ambassador at Large for RStudio, writes monthly posts on the
\href{https://rviews.rstudio.com/}{R Views} blog highlighting
interesting new R packages. Rickert also features special articles about
recently released packages and lists of top packages within certain
categories, including Computational Methods, Data, Machine Learning,
Medicine, Science, Statistics, Time Series, Utilities, and
Visualization.

You can also attend---or access content from---conferences to learn
about improved, recently released, or up-and-coming R package
developments and applications. Two large annual R conferences are
\href{https://rstudio.com/conference/}{rstudio::conf} for industry and
\href{https://www.r-consortium.org/}{useR!} (not exclusively) for
academia. As supportive and innovative subsets of the R community, some
boutique industries that use R host smaller conferences:
\href{https://rinpharma.com/}{R/Pharma} (pharmaceutical development),
\href{https://www.rinfinance.com/}{R/Finance} (applied finance), and
\href{https://www.bioconductor.org/help/events/}{BioC} (open source
software for bioinformatics). Conferences in your field may foster
connections with fellow scientists who use R for similar tasks and help
you collect information about packages related to your expertise. Talks
and presentations at conferences are often recorded and made available
online for playback if you can't attend in person or wish to revisit
content from past conferences. For instance, R Studio has a
\href{https://rstudio.com/resources/rstudioconf-2020/}{webpage} devoted
to videos of past conferences by year and the talks from useR! are
posted on the R Consortium
\href{https://www.youtube.com/channel/UC_R5smHVXRYGhZYDJsnXTwg/featured}{YouTube
channel}. Of late, the R community and rOpenSci have popularized
nontraditional Unconferences and R Collaboratives at which people
interact and share ideas in a collegial and unstructured atmosphere; as
you may imagine, R packages are a hot topic of conversation at these
events.

Don't forget, a developer of an R package may intend for it to be
private (exclusively for personal or professional use) or public (at no
cost and available for use by anyone) {[}23{]}. If your task is specific
to a line of research, consult colleagues to see if they have a relevant
(private) package they would be willing to share.

\hypertarget{rule-3-check-how-its-shared}{%
\section{Rule 3: Check how it's
shared}\label{rule-3-check-how-its-shared}}

Packages can be shared through a variety of platforms. Repositories are
a primary way in which developers share packages for both public and
private use, but there are alternatives. As far as R packages are
concerned, a repository is essentially a warehouse for tools before they
are shipped to your kitchen; in computing terms, a repository is
analogous to a cloud because it is a central location in which data is
stored and managed. Some repositories impose vetting mechanisms that
tame unwieldy aspects of the R ecosystem by regularly checking
underlying code (not just R code) and managing corresponding webs of
dependencies. Two traditional repositories for R packages are CRAN and
Bioconductor; however, there are lesser-known remote repositories that
have unique properties. Developers can also share their R packages
through large code-sharing sites, including public version control
platforms like GitHub and GitLab; these have fewer restrictions on the
format or content of shared code compared to repositories. At the least
public level, in lieu of making packages accessible to everyone via
internet repositories, some developers share their code in zipped files
directly with collaborators (see Rule 2).

The CRAN package repository is the most established and primary source
from which you can install R packages. Volunteers from the R Core Team
(est. 1997) host and maintain a monumental collection of R packages on
CRAN (over 16,000 as of August 2020) on almost any conceivable topic,
from random forests to multidimensional maps {[}9,24,25{]}. Due to its
longevity and historical role, Rickert asserts that ``CRAN is the
greatest open source repository for statistical computing knowledge in
the world'' {[}23{]}. A key advantage of using a package from CRAN is
that the repository integrates easily with your base R installation
{[}26{]}. You can simply use the \texttt{install.packages()} function to
install a package from CRAN, along with all of the packages it depends
on; source and/or binary code are automatically saved to your computer
in a designated package library {[}11,27{]}. These features silently
enable seamless ease of use. When you want to use a particular package,
you load it to your R session via the \texttt{library()} function from
base R {[}11{]}. The R Foundation manages CRAN and imposes some strict
regulatory practices for the selection and maintenance of the packages
they host, particularly as standards pertain to the structure of a
package. CRAN is more or less mechanical in their approach to accepting
and keeping a package as it evolves in the R ecosystem. CRAN is not as
concerned with content or substance because they do not have a peer
review process (see Rule 3 and Rule 6). At worst, a CRAN package might
have a solid framework with help files for each function, but its
underlying code might not necessarily do what it claims. A package must
pass a series of initial stability tests in accordance with the CRAN
Repository Policy before obtaining publication privileges {[}28{]}. A
number of CRAN packages make impactful contributions to certain subject
areas including statistical computing and graphics; however, some CRAN
packages, such as \texttt{weathermetrics} for converting between weather
metrics, are more convenient than substantial {[}29{]}. Volunteer CRAN
maintainers actively monitor portions of code (e.g., vignettes, help
files, test directories) for source and contributed packages to ensure
compatibility with the latest version of R and dependencies; they modify
or remove packages that do not uphold these publication quality
standards.

If you're looking for tools specifically designed for high-throughput
genomic data, the Bioconductor repository is a more specialized
repository than CRAN that's worth investigating. The Bioconductor
project was motivated by a need for transparent, reproducible, and
efficient software in computational biology and bioinformatics and
supports the integration of computational rigor and reproducibility in
research on biological processes {[}7{]}. Bioconductor packages
facilitate the analysis and comprehension of biological data and help
users solve problems that arise when working with high-throughput
genomic data such as those related to microarrays, sequencing, flow
cytometry, mass spectrometry, and image analysis {[}30{]}. Bioconductor
boasts a modularized design, wherein data structures, functions, and the
packages that contain them have distinct roles that are accompanied by
thorough documentation. Accessibility is a pillar of the Bioconductor
project; all forms of documentation, including courses, vignettes, and
interactive documents, are curated for individuals with expertise in
adjacent disciplines and minimal experience with R (see Rule 4) {[}7{]}.
Bioconductor packages integrate well with base R by different means than
CRAN packages; you should use \texttt{BiocManager::install()} to install
Bioconductor packages with their corresponding dependencies {[}31{]}.
Bioconductor has a structured release schedule and thus a unique package
management system which automatically loads your Bioconductor packages
in versions that are all from the same release cycle and compatible with
your current version of R {[}30{]}. The R environment and its package
system is fundamental to the implementation of Bioconductor's
interoperable and object-oriented (S4) infrastructure. The tasks for
which Bioconductor packages provide tools generally involve large and
complex datasets that need to leverage R's object oriented programming
functionalities---a set of specific object classes in particular (e.g.,
ExpressionSet class) {[}32{]}. This helps, among other things, keep
metadata properly aligned with sample data. Similar to CRAN,
Bioconductor has strict criteria for package submissions: a package must
be relevant to high-throughput genomic analysis, interoperable with
other Bioconductor packages, well-documented, supported in the
long-term, and comply with additional package guidelines {[}33{]}.
Unlike CRAN, however, to ensure Bioconductor software maintains a
centralized content focus, their packages are coordinated and
peer-reviewed.

Additionally, there are smaller, specialized repositories that share R
packages. The non-profit organization, rOpenSci, for example, runs a
repository as part of their commitment to promote open science practices
through technical and social infrastructure for the R community
{[}34{]}. The repository only includes packages that have either been
developed by staff to fill a gap in infrastructure or have passed their
open review process. These packages are within the scope and aims
defined by rOpenSci, including those that focus on data extraction,
workflow automation, and field or laboratory reproducibility tools
{[}35{]}. R package maintainers can even create and host their own
personalized repository using the \texttt{drat} (Drat R Archive
Template) package, which enables developers to design individual
repositories and suites of coordinated repositories for packages that
are stored in and/or distributed through platforms like GitHub {[}36{]}.
In some cases, this might be done to host a package that does not meet
certain restrictions from other repositories (e.g., to share data
through a package that is larger than CRAN's size limit for packages
{[}37{]}). There are other platforms strictly for the development,
rather than distribution, of R packages such as R-Forge and Omegahat,
which are beyond the scope of this paper {[}38,39{]}.

While there are evident benefits of exclusive repositories, there are
also considerable advantages to hosting a package on online hosting
platforms for git repositories like GitHub or GitLab {[}23,40{]}. These
platforms can be used for production of R packages, including
large-scale packages. Package developers can post the directory with all
source code for their package as they develop it, from the earliest
stages. The platforms allow for continuous integration and continuous
deployment for testing data and code to ensure a stable end-product. One
particularly popular platform when it comes to sharing R packages is
GitHub. An increasing number of packages are hosted on GitHub during the
development stages, and so it will often be the first place you can find
up-and-coming packages (see Rule 2). Even if developers choose not to
distribute their package through GitHub, the stable release versions of
such packages are often published on CRAN or Bioconductor {[}41{]}, and
for these GitHub can be a place to check out planned changes for the
next stable version that the developer plans to publish. GitHub provides
R users with open access to package code, a timeline of help resources
(see Rule 4), and a direct line of communication to developers. It is
simple to install the latest version of a package you want to use from
GitHub via
\texttt{devtools::install\_github("DeveloperName/PackageName")};
however, the decentralized nature of GitHub is not conducive to a tool
that automatically locates and installs corresponding dependencies that
are not on CRAN {[}42{]}. In fact, the rapid uptick in package
development and ensuing inter-repository dependencies has sparked an
ongoing debate on whether regulated repositories such as CRAN and
Bioconductor are preferable to distribution platforms like GitHub due to
this complication {[}23,40,43{]}. Further, when a package is installed
from GitHub, typically only the source code is available, not binaries
pre-built for your computers operating systems. This means that the
installation process will need to build the package from source on your
computer, which may require certain compilers and other tools (e.g.,
XCode for macOS, Rtools for Windows). Both R users and package
developers benefit from interactive feedback channels through GitHub
Issues and the Star rating system.

When you are searching for and selecting packages, it is important to
note that a single package is often available in multiple places.
Packages that are on CRAN, Bioconductor, or rOpenSci are usually on
GitHub. There is an unofficial read-only GitHub mirror at CRAN,
\href{https://github.com/cran}{METACRAN}, in which \emph{all} CRAN
packages have their own GitHub repositories with all versions, original
dates, and authors. Additionally, package developers often use their
personal GitHub accounts to publicize a package while it is being
developed and continue to host development versions after it has been
published on formal repositories such as CRAN. In this case, CRAN would
host a ``stable'' version of the package, which may simultaneously be in
the ``master'' branch of the developer's GitHub repository for the
package; forthcoming package updates would remain in a ``development''
branch in the same GitHub repository. Although less relevant to new R
users, this means that you can access and test upcoming versions of a
package through GitHub, which may be particularly helpful if you're
developing a package with dependencies and want to know if upcoming
changes to a package that your code depends on will cause issues.
Lastly, there are some instances where a packages is posted on
CRAN---not only because CRAN is its main repository, but also to ease
its connection with base R---then also published in a specialized
repository like rOpenSci. As part of their review process, rOpenSci asks
authors of submitted packages if they plan to publish on CRAN (or
Bioconductor) and many times packages become available in both places.
If you want a package that more or less guarantees that it does what it
says it will do, consider packages that have been peer reviewed. Knowing
this, you might start by searching ROpenSci's repository website rather
than CRAN Task Views or biocViews.

\hypertarget{rule-4-explore-the-availability-and-quality-of-help}{%
\section{Rule 4: Explore the availability and quality of
help}\label{rule-4-explore-the-availability-and-quality-of-help}}

There has been a call for the development of centralized resources in
statistical computing to enable a common understanding of software
quality and reliability: software information specified in publications,
domain-specific semantic dictionaries, and a single metadata resource
for statistical software {[}10{]}. No such resources have been
consolidated to serve these purposes and, given the decentralized nature
of today's information society, it is questionable whether they will
emerge. Current sources of information related to R packages are
dispersed and plentiful. On one hand, this allows users to explore
diverse solutions and discover new tools; on the other, not knowing
where to find help can lead to inefficient and ineffectual roundabouts.
Clearly, not all package resources share the same level of quality and
the fact that there are many resources in aggregate does not imply that
every package is associated with the same availability of resources.
While all R packages warrant some minimal standard of documentation,
beginners and users of complex packages might desire more.

Help associated with R packages generally falls into one of two
categories: help documentation that comes embedded in the source code of
the package (i.e., ships with the package) and documentation or other
resources that are available more broadly, beyond the package's source
code. The documentation that ships with the package can consist of help
files, vignettes, \texttt{README} files, tutorials, and \texttt{pkgdown}
documentation {[}44{]}. Broader help resources can include webpages or
online books whose content is hosted outside the package source, online
tutorials, electronic mailing lists, and large and vocal communities of
users. Implicitly, if an author has taken the time to provide copious
and useful help documentation---within package code, outside of it, or
both---this indicates that the authors are at least serious about their
package development and adds confidence that they know what they (and
their package) are doing. Exemplary documentation can signify an
exceptional package, while you'll often want to avoid using a package if
the quality and quantity of related resources is lacking.

At the most granular level, every R package should come with a help file
for each function that the package maintainer intends for you to use
directly (as opposed to smaller help functions meant to solely be used
within other package functions). This practice is required for packages
on some repositories, like CRAN, but might not be adopted for packages
posted on less restrictive platforms, like GitHub. For each function in
the package, you can access its help file from the console by typing
\texttt{?} followed by the function's name (e.g., to get the help file
for the \texttt{mean} function, type \texttt{?mean}). Function help
files provide information about how to use the function, including the
parameters that can be set, the format to use when specifying arguments
for these parameters, a description of the file, and even references to
papers or other resources on which the function's underlying algorithm
was based. They often contain executable examples to help you get a feel
for how to use the function in practice. If you would like to check all
help documentation for a package, you can access an index of help pages
from the console via \texttt{help(package\ =\ "...")} {[}11{]}. The
documentation that accompanies functions within packages is critical;
the fact that anyone can read the documentation anytime and use it to
guide their own work facilitates extensibility. The extent and quality
of content in these help files will vary; ideally, packages should have
thorough documentation at the function level. If the help pages alone
leave you wanting, the package likely does not have further (quality)
documentation and therefore should not be your first choice, if
comparable options exist. In short, if the developer cannot initially
communicate how their tool works, then you may not want to use it in
your kitchen (read: if the instruction manual is useless, don't use the
blender).

Fortunately, plenty of R packages include additional documentation
beyond this minimum. While the function-level help files can be rather
technical and extraneous to new users, a vignette is a practical type of
documentation in the form of a tutorial. A vignette is a detailed,
long-form document that describes the problems an R package can solve,
then illustrates applications through clear examples of code with
coordination of functions and explanations of outcomes. See, for
example, the extensive vignette for the
\href{https://cran.r-project.org/web/packages/sf/vignettes/sf1.html}{\texttt{sf}
package}{[}45{]}. Packages can have multiple vignettes; you can view or
edit a specific vignette or obtain a list of all vignettes for a package
of interest via the \texttt{vignette()} function from base R {[}11{]}.
For a package shared through CRAN or Bioconductor, a list of the
package's vignettes can also be found on the CRAN or Bioconductor
homepage for the package. Particularly for packages hosted on GitHub,
developers will create a mini-vignette in the \texttt{README} to provide
a high-level overview of their package and its components, along with a
basic introduction and short tutorial for its usage. This mini-vignette,
displayed on the package repository's main page, should simply tell you
why you should use it, how to use it, and how to install it {[}27{]}.

Tutorials, most commonly those powered by the
\href{https://rstudio.github.io/learnr/}{\texttt{learnr} package}, are
another effective and popular way to facilitate learning about R
packages {[}46{]}. They are either conceptual and exploratory or
specific and structured in their content and can include a range of
features: narratives, figures, videos, illustrations, equations, code
exercises, quizzes, and interactive Shiny elements. You can run
\texttt{learnr::run\_tutorial(name\ =\ "\ldots{}",\ package\ =\ "\ldots{}",\ shiny\_args\ =\ "\ldots{}")}
to deploy the \texttt{learnr} tutorial included within a package, if one
exists {[}46{]}. Alternatively, as of RStudio v1.3, you can access
\texttt{learnr} tutorials for packages you've installed, directly from
the Tutorial pane in the IDE {[}47{]}. Some developers publish their
\texttt{learnr} tutorials as Shiny apps, which can be called from within
the \texttt{learnr::run\_tutorial} function and accessed outside of the
RStudio interface {[}46{]}.

Several thousand developers have been using \texttt{pkgdown} to create
static HTML documentation {[}44,48{]}. This tool allows developers to
collect dispersed forms of documentation for their package such as help
files and vignettes, then build a single package website with this
information plus more. The
\href{https://pkgdown.r-lib.org/}{\texttt{pkgdown}} website itself as
well as those for
\href{http://naniar.njtierney.com/index.html}{\texttt{naniar}},
\href{https://rnabioco.github.io/valr/}{\texttt{valr}}, and
\href{https://xavierdidelot.github.io/TransPhylo/articles/TransPhylo.html}{\texttt{TransPhylo}}
are examples of \texttt{pkgdown} documentation in action for R packages
{[}44,49--51{]}. Sometimes, you can find a link to a package's
corresponding \texttt{pkgdown} website in the ``URL'' section of its
CRAN page, which is often listed alongside its GitHub link.

So far, we've only mentioned types of help documentation that are
incorporated in the source code of a package; you can investigate them
by looking through a package's underlying code. However, there are
easier avenues for accessing help documentation encoded in a package.
RDocumentation, for example, is a searchable
\href{https://www.rdocumentation.org/}{website}, package
(\texttt{install.packages("RDocumentation")}), and
\href{https://www.rdocumentation.org/docs/}{JSON API} for obtaining
integrated documentation for packages that are on CRAN, Bioconductor,
and GitHub {[}11,52{]}. RDocumentation may include: an overview,
installation instructions, examples of usage, functions, guides, and
vignettes. This is a subsidiary reason why is may be useful to look for
packages shared on these platforms (see Rule 3).

Some packages have a comprehensive set of resources beyond those encoded
in the package source code. For instance, there are online and/or print
books associated with certain packages. One popular method that
developers use to publish books about their package is through
\texttt{bookdown}, a relatively new extension of R Markdown that is
structured in such a way that integrates code, text, links, graphics,
videos, and other content in a format that can be published as a free,
open, interactive, and downloadable online book {[}53{]}. The
\texttt{bookdown} package itself has an
\href{https://bookdown.org/yihui/bookdown/}{online book} that details
usage of the package {[}53,54{]}.

Other help documentation that is shared outside of the package's code
includes online courses, galleries, and cheatsheets. Some packages have
corresponding online courses, which might be taught by the maintainer.
\href{https://noamross.github.io/gams-in-r-course/}{\emph{GAMs in R}},
for instance, is a free interactive course on Generalized Additive
Models in R using the \texttt{mgcv} package {[}55{]}. Every now and
then, an online course will cover one or a few select packages that
revolve around a central theme such as a types of modeling. A suitable
example,
\href{https://supervised-ml-course.netlify.app/}{\emph{Supervised
Machine Learning Case Studies in R}}, features several \texttt{tidy}
tools with a primary focus on \texttt{tidymodels} {[}56{]}. Galleries,
as the name would imply, are sites that display works, articles, and/or
code examples associated with certain packages. As you might expect,
galleries can be valuable sources of inspiration, particularly for
packages that provide tools for data visualization tasks. See, for
example, \href{https://www.r-graph-gallery.com/}{The R Graph Gallery} as
well as the galleries for
\href{https://exts.ggplot2.tidyverse.org/gallery/}{\texttt{ggplot2}
extensions},
\href{https://rstudio.github.io/dygraphs/index.html}{\texttt{dygraphs}
for R}, \href{https://shiny.rstudio.com/gallery/}{\texttt{shiny}}, and
\href{https://gallery.htmlwidgets.org/}{\texttt{htmlwidgets}} for
organized displays of outputs from these packages {[}57--59{]}. The
\href{https://gallery.rcpp.org/}{\texttt{Rcpp} gallery} is an instance
of a gallery that contains collections of articles and code examples for
the \texttt{Rcpp} package {[}60{]}. RStudio hosts cheatsheets of concise
usage information for popular packages through code and graphics
organized by purpose, including cheatsheets made by RStudio's team and
those contributed by others. Cheatsheets can be accessed directly via
the RStudio Menu (Help \textgreater{} Cheatsheets) or from the
\href{https://rstudio.com/resources/cheatsheets/}{RStudio website} on
which you can subscribe to cheatsheet updates and find translated
versions. Packages with their own cheatsheets include: \texttt{caret}
(statisical modeling and machine learning), \texttt{data.table} (data
manipulation), \texttt{devtools} (package development),\texttt{dplyr}
(data transformation), \texttt{ggplot2} (data visualization),
\texttt{leaflet} (interactive maps), \texttt{randomizr} (random
assignment and sampling), \texttt{sf} (tools for spatial data),
\texttt{stringr} (character string manipulation), and \texttt{survminer}
(survival plots) {[}12,42,45,57,61--66{]}. Further information about
some R packages is available in video tutorials, webinars, and code
demonstrations (i.e., ``demos'').

While using a package, anticipate complications beyond the scope of
documentation. In this case, you will use resources that involve
\emph{asking} for help---should the occasion arise, you want to be
assured that you will find a satisfactory answer. The R-help mailing
list, to which you can subscribe and send questions, is moderated by the
R Core Development Team and includes additional facets for major
announcements about the development of R and availability of new code
(R-announce) and new or enhanced contributed packages (R-packages)
{[}67{]}. In the early days, the R-help mailing list was the only way to
seek direct assistance; since, the R community has evolved to make
asking for help more widespread and accessible, with inclusion and
creative problem-solving as hallmarks of its online presence {[}68{]}.
Certain packages have independent mailing lists; \texttt{statnet} is an
example of a suite of packages that has its own
\href{http://statnet.org/}{community mailing list} {[}69{]}. If a
package has a development repository on GitHub, check the Issues to
verify that the maintainer is responsive to posts and fixes bugs in a
timely manner. In addition, you can search discussion forums such as
Stack Overflow, Cross Validated, and Talk Stats to assess the activity
associated with the package in question. Analyses of the popularity of
comparable data analysis software in email and discussion traffic
suggest that R is rapidly becoming more prevalent and is the leading
language by these metrics {[}6,70{]}. When you encounter a problem, it
is good practice to first update the package to see if the problem is
due to a bug in a previous version---if the problem persists, seek help
by finding or posting a reproducible example {[}71{]}.

One example of a package---mentioned a few times already---with notable
documentation and first-rate help resources is \texttt{Rcpp} {[}60{]}.
The developers maintain both a \href{http://www.rcpp.org/}{main} and
\href{http://dirk.eddelbuettel.com/code/rcpp.html}{additional} website
with a wealth of organized information about the package and resources,
including a gallery of examples, associations, publications, articles,
blogs, code, books, talks, and links to other resources with
\texttt{Rcpp} tags. The package also has a dedicated
\href{https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/rcpp-devel}{mailing
list}, for developers using the package and the package is thoroughly
described in a print book by its maintainer {[}72{]} and in chapters or
sections of other books (e.g., {[}71{]}).

\hypertarget{rule-5-quantify-how-established-it-is}{%
\section{Rule 5: Quantify how established it
is}\label{rule-5-quantify-how-established-it-is}}

Consulting data to inform comparisons is never a bad idea; numerical
data associated with R packages will give you an impression of how
popular a tool is and whether it has stood the test of time. Since there
are tens of thousands of R packages, you may be wondering how they stack
up in terms of popularity. There are metrics that you can use to
determine how often an R package has been downloaded, how long it's been
around, and how often (and recently) it's updated through new versions.

You can find several of these metrics by visiting the site(s) on which a
package is shared (see Rule 3). If a package is shared through a GitHub
repository, the homepage for the repository will list the number of
GitHub Stars, Forks, and Watchers for that particular repository. As
proxies for a repository's overall following, and possibly indirect
measures of quality and impact, Stars indicate that people like or are
interested in a project, Forks reveal the number times a repository has
been copied, and Watchers represent GitHub users who are subscribed to
updates on a repository's activity {[}73{]}. Although not a definitively
unbiased metric, a large number of Stars, Forks, and Watchers associated
with a package implies a substantial following and widespread usage
{[}74{]}. If a package is posted on CRAN or Bioconductor, its page on
that site will include the date on which its latest version was released
as well as a link to ``Old sources''---previous versions of the package
and the release date of each. The page also provides a link to a
\texttt{NEWS} file, where you can see a list of all published versions
as well as the major changes made from one version to the next. To
further determine if a package is well-established in the R community,
refer to the number of versions and updates (more is better) in addition
to the date of the most recent versions and updates (newer is better).
You can also see, on a package's CRAN or GitHub page, the number of
reverse dependencies it has. When a package has numerous reverse
dependencies, the code underlying many other R packages incorporates
functions from that package, which can indicate that other R developers
find that package useful.

RDocumentation (see Rule 4) is rich with stats on R packages, uniting
many of the previously discussed metrics, while also providing
cross-package summaries. RDocumentation hosts a live
\href{https://www.rdocumentation.org/trends}{Leaderboard} with trends
including the number of indexed packages and indexed functions, most
downloaded packages, most active maintainers, newest packages, and
newest updates. What's more, each package is assigned a percentile
rank---featured on its RDocumentation page---that quantifies the number
of times a package has been downloaded in a given month. A ranking
algorithm computes the direct, user-requested monthly downloads by
accounting for reverse dependencies (indirect downloads) so packages
that are commonly used within other packages, and hence frequently
downloaded as automatic downloads with those packages, do not skew the
calculation {[}75{]}. You can research stats on corresponding
dependencies for a more holistic picture.

Most of the above-mentioned resources will help you learn more about
packages on an individual basis. Wouldn't it be nice if you could
compare related packages that may have features that overlap? Wouldn't
it be even nicer if you could use an R package to easily do so? You're
in luck; rOpenSci developed
\href{https://docs.ropensci.org/packagemetrics/}{\texttt{packagemetrics}:
A Package for Helping You Choose Which Package to Use} {[}76{]}. Fun
fact: the \texttt{packagemetrics} project was born at an Unconference
{[}77{]}. You can install \texttt{packagemetrics} via
\texttt{devtools::install\_github("ropenscilabs/packagemetrics")} and
run the \texttt{packagemetrics::package\_list\_metrics()} function where
the input is a vector of packages of interest, by name {[}42,76{]}.
Then, you can use \texttt{packagemetrics::metrics\_table()} to obtain a
table of information that includes each package's respective number of
downloads, GitHub Star rating, \texttt{tidyverse} compatibility, and
several more indicators to help you decide which package is most
established and widely used (\textbf{Figure of table output {[}BA: That
could be great, or even use a Box to show an example of running the code
you just described and the resulting output.{]}}) {[}14,76{]}.

An important caveat to consider: several of the metrics introduced in
this Rule are based on how often a package is downloaded; however, it is
not uncommon for users to download a package to explore its
functionality and ultimately decide against using it for their task.
Therefore, these suggested metrics can provide a first pass at
identifying popular packages---and will, in particular, help you
distinguish popular packages for common general tasks, like data
wrangling and visualization---but may be less helpful when you are
selecting a package for a very specialized task. In this case, evidence
of peer acceptance and review will usually be more instructive, which
brings us to our next Rule.

\hypertarget{rule-6-seek-evidence-of-peer-acceptance-and-review}{%
\section{Rule 6: Seek evidence of peer acceptance and
review}\label{rule-6-seek-evidence-of-peer-acceptance-and-review}}

Peer review is an important aspect of scientific research, not least
because it establishes scholarly credibility. While scientific articles
undergo a peer review process that is fairly standardized across
disciplines, R packages have a broader range of possibilities for peer
review or other evidence of acceptance by members of the scientific
community. You can research information about an R package in different
forms of literature and determine the extent to which it has been
validated by the scientific community.

First, some maintainers of select packages have written an article
describing their package and its algorithms; such articles must go
through the traditional peer-review process in order to be published.
Certain journals publish articles about R packages themselves while
others feature work that used a particular package. Journals with a
targeted and technical focus on R packages and other open-source
scientific software include the
\href{https://joss.theoj.org/}{\emph{Journal of Open Source Software}},
the \href{https://www.jstatsoft.org/index}{\emph{Journal of Statistical
Software}}, and \href{https://journal.r-project.org/}{\emph{The R
Journal}}. Discipline-specific journals like
\href{https://bmcbioinformatics.biomedcentral.com/}{\emph{BMC
Bioinformatics}}, may also publish articles to introduce R packages that
have proven to be useful for aspects of the bioinformatics research
process, for example. Try searching various websites for prominent
journals in your field with queries that include \texttt{"R\ package"}
to find these types of articles. Note, the number of Google Scholar
citations for a package or, similarly, an article about a package,
serves as a metric for a package's impact on scientific research and
utility in research contexts (see Rule 5).

For an R package featured in a peer-reviewed journal article, you can be
assured that the ideas and principles behind the package have been
evaluated by outside reviewers. However, such a review may not have
included a fine-grained peer review of the underlying code in the
package itself. There is an expanding movement toward direct peer review
of R packages at the code level. More specifically, rOpenSci is a unique
example of an ecosystem of open source tools with peer reviewed R
packages (see Rule 3) {[}34{]}. The rOpenSci organization oversees peer
review of R packages at a highly detailed level that is increasingly
desired by scientists; packages published through their repository have
passed external review of the code. They limit their scope to packages
that help scientists implement reproducible research practices and
manage the data lifecycle {[}35{]}. Their review process is conducted
openly on GitHub and, as a result, the scientific community can view the
discussions between package maintainers and peer reviewers that form
this transparent process.

You can also seek evidence of peer acceptance of a package by finding
out how often researchers have used it within scientific papers
describing research, rather than package-focused papers. Useful packages
may be mentioned in the Methods and/or References sections of such
papers. Packages hosted on CRAN or Bioconductor have a standardized
format for citation, and a call to \texttt{citation("PackageName")} will
print the suggested BibTex citation for the package {[}11{]}. Further,
in response to the rising number of researchers creating tools and
software to work with their data, GitHub has granted developers the
ability to obtain a Digital Object Identifier (DOI) for any GitHub
repository archive so that code can be cited in academic literature
{[}78{]}. You can search for packages directly by name in Google
Scholar: the \texttt{Cited\ by} link displays the number of times a
package has been cited and connects to a page with those publications
base on citations using these formats. Further, if a package has such a
DOI, you can explore the network of research associated with that
package. While it is best practice to cite R packages using these
citation formats, occasionally R packages will be mentioned in the
Methods section of a paper without a formal citation.

Finally, many R packages are associated with content in books and series
from scientific publishers. For example, Springer has a
\href{https://www.springer.com/series/6991}{\emph{Use R!} series} with
books covering specialized topics from finance to marketing, from
chemometrics to brain imaging, and more. Additionally, consider reading
titles from
\href{https://www.routledge.com/Chapman--HallCRC-The-R-Series/book-series/CRCTHERSER}{\emph{The
R Series} at Chapman \& Hall/CRC}. The books in this series span topics
related to R in terms of its applications (e.g., epidemiology,
engineering, social sciences), uses (e.g., statistical methods), and
development (e.g., creating packages).

\hypertarget{rule-7-find-out-who-developed-it}{%
\section{Rule 7: Find out who developed
it}\label{rule-7-find-out-who-developed-it}}

Just as research is a library of shared insight, open source software is
a collection of shared tools. We care about who writes the articles we
read; we should also care about who creates the tools we use. Although R
is grounded in statistical computing and graphics, there is variation in
R users' backgrounds and skills, and the same is true for R developers.
That said, the R community prides itself on embracing newcomers at all
levels of involvement. This Rule does not imply that worthwhile packages
are exclusively written by well-known authors. Rather, associations and
reputation can be a proxy for quality; in this way, the process of
evaluating and comparing R packages is no different than other
decisions. In fact, as you become more immersed in the R community, you
will find that name recognition is a factor, among many, that helps you
establish trust in certain tools more quickly than others {[}74{]}.

Information about the package authors can be found in the metadata of
the package. If you are looking at the source code of the package, there
should always be a \texttt{DESCRIPTION} file with metadata. The
\texttt{DESCRIPTION} is a succinct record of the package's purpose,
dependencies, version, date, license, associations, authors, and other
technical details. In the ``Author'' field of this file, you can
identify both the ``Maintainer'' of the package (a similar role to that
of the corresponding author for an article) as well as other authors or
contributors. For packages hosted solely on a platform like GitHub, this
might be the only way to discover a package's main author, but profile
links for all authors are available under the Contributors section on
the package's main GitHub page. On the other hand, for packages hosted
on repositories, this package data is usually also posted by the
repository somewhere online. Packages on CRAN have a page of metadata
that is accessible through a standard web address. For example, to see
the metadata page for the \texttt{ggplot2} package, you can go to
\url{https://CRAN.R-project.org/package=ggplot2}; replace the package
name at the end of the web address with the name of any other CRAN
package to access analogous package-specific metadata {[}57{]}. Here,
you might notice ORCiD links and/or various abbreviations associated
with the names in the ``Author'' and Maintainer" fields that denote
roles such as author \texttt{{[}aut{]}}, creator/maintainer
\texttt{{[}cre{]}}, contributor \texttt{{[}ctb{]}}, and funder
\texttt{{[}fnd{]}}. Similarly, Bioconductor packages each have their own
webpage, with some metadata at the top of the page, including the
maintainer and other authors along with their roles. For example, the
page for the package \texttt{DESeq2} can be found at
\url{https://bioconductor.org/packages/release/bioc/html/DESeq2.html};
you can view pages corresponding to other Bioconductor packages by
altering the web address as previously described {[}79{]}.

You can assess the credibility and commitment of R package developers
through direct and indirect signals. Who made the package? Consider
whether expertise in a certain domain is vital to the design and
creation of the tool. Research the authors' associations in academia,
government, and/or industry and gauge the extent to which they have a
primary role in R development. Sometimes, it's possible to deduce author
affiliations by simply looking at their domain name on their email
addresses. Affiliations may serve as a proxy for package quality and
sophistication insofar as standards, on average, tend to be higher in
regulated settings. R packages produced by the
\href{https://owi.usgs.gov/R/}{United States Geological Survey (USGS)},
for instance, are maintained by domain experts that know field-specific
data sources and algorithms very well. Moreover, authors with
affiliations that include R package development as a paid responsibility
(e.g., USGS, RStudio) might, on average, produce better packages. If a
package was developed by multiple authors, research each of them to
evaluate the robustness of the team. You can learn more about their
experience, active contributions to the R community, and history related
to package development by exploring their profiles on GitHub, Google
Scholar, Research Gate, Twitter, or personal or package websites.
Frequent commits and effective resolutions of GitHub Issues can be a
testament to the authors' priorities and commitment. If an author has a
history of package development, peruse their portfolio of packages to
see if any are highly regarded, recognizable, or impressive at first
glance. When you use a great tool in your kitchen, you consider buying
more tools from that brand. If, in the past, you found a package that
you like to use and is helpful for your work, it can be worthwhile to
research other packages made by that author and/or maintainer. You might
benefit from making a habit of paying attention to who makes tools that
work well for your needs and are designed in a way that seems intuitive
to you---take note of these ``favorite'' package authors.

As you may know, the R community values collaboration---it is not
uncommon for packages to be created and maintained by more than one
person and, in many cases, large teams. Roles within these teams can
shift as the package evolves. As an example, \texttt{ggplot2} has no
small team of authors and its original maintainer is not the same as its
current one {[}57{]}. Oftentimes, existence of a large team can give you
clues (although not guaranteed) about how the package is evolving in the
R ecosystem. For one, it could indicate that a package is very useful
and has generated rich community involvement, as is the case for
\texttt{ggplot2}---many of its authors were added after the first
release {[}57{]}. This might reveal that a package is part of a
larger-scale and more organized project. Also, a sizeable team could
imply that the package grew out of a substantial collaboration, like a
funded grant with a multi-person team; this happens a lot for specific
methods packages. The \texttt{flowCore} package is one example of such a
case {[}80,81{]}. This might suggest that a package has a certain level
of review and investment from several R programmers (the authors), not
just one, as in a solo-authored project. It's also useful to examine the
full list of authors (even if it's long) because you might not recognize
the maintainer's name, but you might recognize another author's name.
This can happen if the maintainer is a student or someone else that's
up-and-coming; sometimes, more senior authors do not assume the role of
maintainer, even though they are quite invested in the project. It's
worth looking to see, for example, if someone who is well-known for
excellence in methods development is one of the non-maintaining authors,
which may help you gauge the quality of the fundamental algorithms and
techniques in the package code. By extension, the indicators of
developer involvement and reputation mentioned in this Rule will help
you discern whether a package is worthy of your trust or requires
evaluation based on other Rules.

\hypertarget{rule-8-see-how-its-developed}{%
\section{Rule 8: See how it's
developed}\label{rule-8-see-how-its-developed}}

You don't need to be a software engineer to identify strong package
development. Scientific software developers sometimes neglect best
practices; indeed, these shortcomings are evident in the tools they
create {[}82{]}. There are concrete ways to measure a tool's robustness
beyond whether it works for those who did not create it. As a user,
there are four main relevant package development protocols---in addition
to some related indicators---that you can investigate to assess the
underlying stability and utility of a package: dependencies, unit
testing, version control, and continuous integration.

R packages often depend on other R packages; you should check the
reputations of such dependencies when selecting a package---quality
packages will rely on a solid web of quality packages. What's more, like
other types of software, well-maintained R packages have multiple
versions corresponding to iterative releases to indicate that the
package is compatible with dependencies and loyally updated (e.g., bug
fixes, general improvements, new functionality) {[}1,27{]}. You can
explore the version history of a package to see if it is up-to-date (See
Rule 5). Package dependencies are not only important to establish
stability, but might also give you a hint as to whether a package will
fit well into your workflow. This is an especially critical
consideration if you're using a ``tidy'' workflow or working with
special object types in, for example, a Bioconductor workflow. You can
review the list of dependencies for a package in question to check if it
depends on some or many of the packages in your workflow. If this is the
case, you can be fairly confident that the package uses the same (or at
least similar) style and conventions as those you want to use.

A responsible developer with a consistent and reproducible workflow will
implement formal testing on their code to examine expected behavior via
an automated process called unit testing {[}27,83{]}. This helps assure
the developer (and you) that the package's underlying units of code do
what they intend to do. Although inconvenient at the outset, the
developer---and by extension, the package user---will benefit from unit
testing, which results in fewer bugs, a well-designed code structure, an
efficient workflow, and robust code that is not sensitive to major
changes in the future {[}27{]}. To alleviate the burdens of unit
testing, \texttt{testthat} is a popular, integrative R package that
helps developers create reliable functions, minimize error, and
visualize progress through automatic code testing {[}84,85{]}.
Developers are also interested in quantifying the amount of code in
their package that has been tested. Test coverage, a measurement of the
proportion of code that has undergone unit testing, is an objective
metric for package developers, contributors, and users to evaluate code
quality. Many developers use the \texttt{covr} package to generate
reports and determine the magnitude of coverage on the function, script,
and package levels {[}86{]}. You can look at the source code for a
package to confirm it has a testing suite; for instance, if you are on a
package's GitHub page, you can navigate to the main Code page and look
for a folder labeled \texttt{tests} or something similar.

Relatedly, developers who host their packages on GitHub, post status
badges, most often, in the overview (\texttt{README}) section of the
repository webpage for the master branch. GitHub badges are a common
self-imposed method to signal use of best practices and motivate
developers to produce software that is high in quality and transparency
{[}87{]}. They are easily visible, interactive, and increase readability
of the \texttt{README}. You may see, for example, license
(\texttt{license}), version (\texttt{release}), dependency
(\texttt{dependencies}), code coverage (\texttt{codecov}), or build
status (\texttt{build}) badges, all of which are good indicators of
package caliber. What's more, badges have dynamic status tags to
indicate the current status of the particular part of a workflow to
which the badge refers; as an example, a \texttt{build} badge with a
\texttt{passing} status tag could be telling you that the package is
passing continuous integration tests, which you will learn more about
soon {[}73{]}. Badges are also clickable buttons that link to a page
that describes and displays detailed aspects of the metric associated
with the badge. For instance, when you click on a \texttt{codecov}
badge, you'll likely end up on a page with a timeline, graphics, and
information about the percentage and distribution of test coverage for
the package. You can refer to special badges like \texttt{CRAN},
\texttt{Bioconductor}, or \texttt{rOpenSci} to determine which
repository (or repositories) a package is shared on (See Rule 3).

As we mentioned in Rule 3, version control has an essential role in
package development and computational literacy more broadly {[}27,88{]}.
Version control is like a time capsule for your workflow because it
monitors and tracks changes to files as a project evolves, and stores
them as previous versions to be recovered if necessary. In other words,
``version control is as fundamental to programming as accurate notes
about lab procedures are to experimental science'' {[}{[}88{]}; p6{]}.
Git is a decentralized open source version control system that is useful
regardless of whether a project is independent or collaborative
{[}89{]}. GitHub works in conjunction with Git to provide a powerful
structured system to organize and manage components of a project for
others and your future self. A growing number of scientists have
research programs based in GitHub, which has become a revolutionary tool
for productive team science and distributed development efforts
{[}1,90{]}. As you may expect, Git coupled with GitHub is the version
control duo of choice among serious R package developers {[}27{]}. Thus,
if the package you are interested in using is among the thousands hosted
on GitHub, this is evidence that the developer is at least committed to
a logical, open, and reproducible workflow, suggestive of more time
spent designing their tool.

Lastly, developers often implement continuous integration (CI) in
conjunction with using a version control hosting platform like GitHub.
CI refers to a process by which developers frequently update and commit
their contributions to a shared repository and routinely merge all
authors' working copies/branches of a project into the master (main)
branch to monitor compatibility---without permanently merging
everything. Just as you are happy to know that a manufacturer guarantees
their kitchen tool is a unified realization of many working parts, you
should be impressed when a package is set up to use CI. This is
particularly important for packages with a large development team. Use
of CI is a good sign that the components of the package are regularly
checked to ensure that code passes tests and the package builds without
failures {[}91{]}.

\hypertarget{rule-9-put-it-to-the-test}{%
\section{Rule 9: Put it to the test}\label{rule-9-put-it-to-the-test}}

If you are unable to decide whether to use a package based on prior
Rules, test it out. Similarly, if you have narrowed your options, work
with each to highlight differences. Exploring the package and engaging
in trial and error using your skills in context of your goal will
illuminate technical details and solidify any doubts.

At this point, what you have learned about the package should be quite
helpful. If the package has a vignette, this is a great place to start
experimenting with code. Vignettes include many common data science
problems with solutions; you can run the code examples, tweak them, and
compare the outputs. First, try some examples with the example data
provided by the package, then see if you can translate that to working
with your own data. Sometimes, package functions will require inputs to
be in a specific data type (e.g., a dataframe, or a certain type of S4
object) or to have a specific format (e.g., if it's in a dataframe, to
have certain columns with certain names). By moving from the example
data to your own, you can gauge the effort it will take to transform
your data to fit these formats. You can also be on the lookout for
whether certain characteristics of your data (e.g., fields with missing
values) cause complications when using the package's functions.

You can continue on to looking through the collection of help files for
the package. Help files often include executable examples, and you can
again start by testing these with the example data provided by the
package, then move on to trying them with your own data and within your
own workflow. In general, it is essential to know the behavior of
different functions within a package, how they interact, and how outputs
respond to changes in inputs. Suppose you are testing a package with
sparse documentation such that function descriptions often include
``\texttt{\ldots{}}'' and the argument descriptions seem incomplete.
This will be problematic if making a reasonable change to an argument
results in an incomprehensible error for which you cannot find help.
When this happens, you may not want to use the package for your task.

Sometimes packages do not interact well with other packages; a recipe
prepared with an odd combination of tools will not turn out. If you are
interested in working with a certain package but are already using other
packages in your workflow, you will need to verify that they work
together. More precisely, you should check the \emph{interoperability}
of all the packages you want to use. A given package may be highly
specialized and incompatible with certain packages in general, or simply
have a few tolerable quirks for which you can develop workarounds. There
are some packages that are masterful at doing what they are made to do,
yet incongruous with other packages. Such packages might, for example,
use S3 or S4 objects, which are two main approaches developers use to
implement object-oriented programming in R. Many packages for spatial
analysis, as well as those from Bioconductor, tend to use S4 objects to
represent data {[}7{]}. On the other hand, the \texttt{tidyverse}, a
unified suite of packages with an grammatical structure employed within
a ``pipeline'', expects dataframe objects {[}14{]}. Because of this,
when you are working in the \texttt{tidyverse}, you may have to take
extra steps to extract components from S3/S4 objects to re-frame the
data into a dataframe. Some packages already exist to help with this
interfacing task between S3/S4 objects and \texttt{tidyverse} tools. The
\texttt{broom} package, and the bioinformatics analog,
\texttt{biobroom}, aim to alleviate these disruptions by converting
``untidy'' objects in S3/S4 objects into ``tidy'' data in dataframes,
thereby making it easier to integrate statistical functions into the
structure of the \texttt{tidyverse} workflow {[}14,92,93{]}.
Furthermore, the \texttt{caret} package facilitates interoperability for
machine learning packages by providing a uniform interface for modeling
with various algorithms from different packages that would otherwise
have independent syntax {[}61{]}.

\hypertarget{rule-10-develop-your-own-package}{%
\section{Rule 10: Develop your own
package}\label{rule-10-develop-your-own-package}}

What if you can't find a package to solve your data science problem?
Though you may not be the best cook, we all come upon times---of
creativity or necessity---that prompt us to experiment in our kitchens.
Rather than exclusively being a user of packages, you can create
them---more easily than you may think {[}27{]}.

If you decide to write your own R code to perform your task (see Rule
1), a natural next step is to wrap this code into a function that can be
re-used; an R package is basically just a collection of one or more of
these functions. The essence of an R package is a self-contained set of
algorithms---encoded as functions---that can be used in combination with
other self-contained sets of algorithms of different shapes and sizes,
including the set of algorithms shared through base R. The uniquely
structured functions within a package help us implement that knowledge
and weave it into novel scientific work. The reasons why you might want
to create a package are abundant: necessity, innovation,
standardization, automation, specialty, containment, organization,
sharing, collaboration, extensibility, and more.

Whatever your motivation, packages are simply sets of tools; you can
create a package out of any collection of specialty functions. A key
tenant of efficient programming is the ``DRY'' principle---Do not Repeat
Yourself. Functions help you replace copied and pasted code with a
defined, compact piece of code in a single place in a script, which you
ca call throughout in shorthand. They help with efficiency and are
warranted when you repetitiously copy and paste your code while making
slight modifications after each iteration {[}71{]}. Since R packages are
just sets of these functions, they are not solely reserved for
large-scale, comprehensive methods; rather, package development can also
help you learn how to apply proper coding techniques to writing
functions and documentation with reproducibility and collaboration in
mind {[}94{]}. Packages need not be formal nor entirely cohesive. For
instance, personal R packages such as \texttt{Hmisc} and
\texttt{broman}, are comprised of miscellaneous functions which the
creator has developed and frequently uses {[}95,96{]}. They're like the
collection of little tried-and-true tricks and tools you've made to cook
your own recipes in your kitchen.

While personal R packages are virtually unbounded, we'll describe some
of their immediate uses. If you often do the same specific,
discipline-oriented data analysis tasks---for which either a package
doesn't exist or you don't want to rely one that does (See Rule 1)---a
personal R package could help you automate some redundant aspects of
your work. You can even connect components of other tools you're using,
that wouldn't otherwise play nicely together. For example, if you're
work largely involves packages that output S3/S4 object types, but you
want to use standard \texttt{ggplot2} tools to visualize the output, you
will encounter a problem because functions in \texttt{ggplot2} expect a
dataframe as the standard input, not these more complex object types
{[}57{]}. In this case, you might want to write some of your own
functions that extract what you need from the S3/S4 objects and convert
them into a dataframe input type so they are compatible with
\texttt{ggplot2}. Interestingly, this type of occurrence is the precise
motivation behind the widely used \texttt{broom} package {[}92,97{]}.
You can also create visualization tools that are tailored to your
research and include them in your personal R package. If you frequently
design a certain type of plot with your data, you can encode that
process into a function, even if it relies heavily on functions from
other packages. For instance, this could happen if you implement
particular tweaks whenever you make scatterplots with your data; many R
users build custom themes within \texttt{ggplot2} for purposes like
personal or professional branding, which can be wrapped into functions
and contained in a personal R package {[}57,91{]}.

Although you may not anticipate that anyone else will use your tools,
following best practices for package development is still a good idea.
You wouldn't want to craft a useful cooking hack without having a good
way to replicate it when you need to cook the same (or similar) recipe
again. Even if you're the only person who will use your package, be kind
to your future self---as a consumer of shared packages, you know the
inherent benefits of robust software development relative to the quality
of code, data, documentation, versions, and tests {[}82{]}. There's also
no reason not to take advantage of existing resources when coding your
package; all your newfound special tools paired with others' shared
tools can help you make exquisite recipes. There are R packages that aid
in package development (e.g., \texttt{devtools}, \texttt{usethis},
\texttt{testthat}, \texttt{roxygen2}, \texttt{rlang}, \texttt{drat})
{[}36,42,85,98--100{]}---while these packages can provide
industrial-strength support for developing large and complex packages,
they also prove useful even in developing small, private packages. They
make it easier and more efficient, for example, to write help
documentation and unit tests for functions in your package. RStudio's
IDE has a framework called ``Projects'', which can help you establish an
organized framework of directories within an R Project
(\texttt{.Rproj})---specifically designed for creating R packages---that
is straightforward to navigate throughout the package development
process. Tips on using many of these package development tools are
available through resources authored by expert R developers, like
\href{https://r-pkgs.org/}{\emph{R Packages}} by Hadley Wickham
{[}27{]}, while the official resource on writing packages to share
through CRAN is the manual,
\href{https://cran.r-project.org/doc/manuals/r-release/R-exts.html}{\emph{Writing
R Extensions}} {[}101{]}.

Regardless of whether you're writing a package to share, or one for your
own use that you don't plan to publish through a repository, consider
using version control. Among the numerous reasons why version control
will positively impact your workflow, you'll be able to easily share
your project with collaborators, at any point in time. If you decide to
collaborate on a package, even if you just intend for the package to
remain a resource among your group of collaborators, the Project
framework is compatible with distributed development---a feature that
couples well with version control and online platforms for version
controlled repositories, like GitHub. Version control also helps you
maintain a clean directory of code without limiting the process of
making changes to files. All changes are trackable and rewindable so
earlier versions are restorable. You can create offshoots of your main
project (branches) to try new things and incorporate (some or all of)
the changes into the main branch. As you work, keep in mind that within
the open source R community, there is no lack of effective
organizational frameworks to reference; in fact, repositories for many
exemplary packages are readily available on GitHub.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The open source movement is the heartbeat of R; it sustains and enlivens
the growth and improvement of the R ecosystem. Anyone, whether they are
motivated to propel a research program or interested in learning from
data in other contexts, can be an R user---without significant barriers
to entry. A number of R users don't stop there, however. They become R
makers by adding their own contributions so not only they, but new and
existing users and makers, can do more in R. This plays an essential
role in the lifecycle of R's beloved shared tools.

R packages are a defining feature of the language insofar as many are
robust, user-friendly, and richly extend R's core functionality. Some of
the most prominent R packages are a result of the developer abstracting
common elements of a data science problem into a workflow that can be
shared and accompanied by thorough descriptions of the process and
purpose. In this way, R packages have effectively transformed how we
interact with data in the modern day in, perhaps, a more impactful
manner than several revered contributions to theoretical statistics
{[}4{]}. Packages greatly enhance the user experience and enable you to
be more efficient and effective when working with data, regardless of
prior experience.

Nonetheless, the sheer quantity and potential complexity of available R
packages can undermine their collective benefits. Finding and choosing
packages, particularly for beginners, can be daunting and difficult. R
users often struggle to sift through the tools at their disposal and
wonder how to distinguish appropriate usage. These ten simple rules for
navigating the shared code in the R community are intended to serve as a
valuable page in your computing cookbook---one that will evolve into
intuition and yet remain a reliable reference. May searching for and
selecting proper tools no longer spoil your appetite and dissuade you
from discovering, trying, creating, and sharing new recipes.

\hypertarget{table-1-general-packages}{%
\section{Table 1 (general packages)}\label{table-1-general-packages}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(kableExtra)}
\KeywordTok{library}\NormalTok{(knitr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# general packages data}
\NormalTok{gen_pkgs <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{Package =} \KeywordTok{c}\NormalTok{(}\StringTok{"readr"}\NormalTok{, }
              \StringTok{"dplyr"}\NormalTok{,}
              \StringTok{"tidyr"}\NormalTok{,}
              
              \StringTok{"broom[note]"}\NormalTok{,}
              \StringTok{"purrr"}\NormalTok{,}
              \StringTok{"caret"}\NormalTok{, }
              
              \StringTok{"ggplot2"}\NormalTok{, }
              \StringTok{"kableExtra"}\NormalTok{, }
              \StringTok{"rmarkdown"}\NormalTok{),}
  
  \DataTypeTok{Description =} \KeywordTok{c}\NormalTok{(}\StringTok{"Read rectangular data (e.g., csv, tsv, and fwf)"}\NormalTok{, }
                  \StringTok{"Grammar of tidy data manipulation"}\NormalTok{,}
                  \StringTok{"Tidy messy data"}\NormalTok{,}
                  
                  \StringTok{"Tidy model output; convert statistical objects into tidy tibbles"}\NormalTok{, }
                  \StringTok{"Functional programming tools for functions and vectors"}\NormalTok{,}
                  \StringTok{"Framework for predictive modeling"}\NormalTok{,}
                  
                  \StringTok{"System for data visualization"}\NormalTok{, }
                  \StringTok{"Build complex tables and manipulate styles"}\NormalTok{,}
                  \StringTok{"Authoring framework for data science and reproducible research"}\NormalTok{),}
  
  \DataTypeTok{Year =} \KeywordTok{c}\NormalTok{(}\StringTok{"2015"}\NormalTok{,}
           \StringTok{"2014"}\NormalTok{,}
           \StringTok{"2014"}\NormalTok{,}
  
           \StringTok{"2014"}\NormalTok{,}
           \StringTok{"2015"}\NormalTok{,}
           \StringTok{"2007"}\NormalTok{,}
                    
           \StringTok{"2007"}\NormalTok{,}
           \StringTok{"2017"}\NormalTok{, }
           \StringTok{"2014"}\NormalTok{),}
  
  \DataTypeTok{Author =} \KeywordTok{c}\NormalTok{(}\StringTok{"Wickham et al."}\NormalTok{,}
             \StringTok{"Wickham et al."}\NormalTok{, }
             \StringTok{"Wickham & Henry"}\NormalTok{,}
  
             \StringTok{"Robinson & Hayes"}\NormalTok{,}
             \StringTok{"Henry & Wickham"}\NormalTok{,}
             \StringTok{"Kuhn"}\NormalTok{, }
                    
             \StringTok{"Wickham et al."}\NormalTok{,}
             \StringTok{"Zhu"}\NormalTok{, }
             \StringTok{"Allaire et al."}\NormalTok{),}
  
  \DataTypeTok{Documentation =} \KeywordTok{c}\NormalTok{(}\StringTok{"https://readr.tidyverse.org/"}\NormalTok{, }
                    \StringTok{"https://dplyr.tidyverse.org/"}\NormalTok{, }
                    \StringTok{"https://tidyr.tidyverse.org/"}\NormalTok{,}
  
                    \StringTok{"https://broom.tidymodels.org/"}\NormalTok{,}
                    \StringTok{"https://purrr.tidyverse.org/"}\NormalTok{,}
                    \StringTok{"https://topepo.github.io/caret/index.html"}\NormalTok{, }
                    
                    \StringTok{"https://ggplot2.tidyverse.org/"}\NormalTok{,}
                    \StringTok{"https://haozhu233.github.io/kableExtra/"}\NormalTok{, }
                    \StringTok{"https://rmarkdown.rstudio.com/lesson-1.html"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# general packages table}
\KeywordTok{kable}\NormalTok{(gen_pkgs, }\DataTypeTok{format =} \StringTok{"latex"}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# scale}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"scale_down"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# separate rows by category}
\StringTok{  }\KeywordTok{pack_rows}\NormalTok{(}\StringTok{"Data Manipulation"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pack_rows}\NormalTok{(}\StringTok{"Statistical Modeling"}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pack_rows}\NormalTok{(}\StringTok{"Data Visualization"}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# column wrap}
\StringTok{  }\KeywordTok{column_spec}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{width =} \StringTok{"10em"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{column_spec}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DataTypeTok{width =} \StringTok{"20em"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# bold column names}
\StringTok{  }\KeywordTok{row_spec}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{bold =}\NormalTok{ T) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_footnote}\NormalTok{(}\StringTok{"See the biobroom analog in Bioconductor"}\NormalTok{,}
               \DataTypeTok{notation =} \StringTok{"symbol"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{>{\raggedright\arraybackslash}p{10em}>{\raggedright\arraybackslash}p{20em}lll}
\toprule
\textbf{Package} & \textbf{Description} & \textbf{Year} & \textbf{Author} & \textbf{Documentation}\\
\midrule
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{Data Manipulation}}\\
\hspace{1em}readr & Read rectangular data (e.g., csv, tsv, and fwf) & 2015 & Wickham et al. & https://readr.tidyverse.org/\\
\hspace{1em}dplyr & Grammar of tidy data manipulation & 2014 & Wickham et al. & https://dplyr.tidyverse.org/\\
\hspace{1em}tidyr & Tidy messy data & 2014 & Wickham \& Henry & https://tidyr.tidyverse.org/\\
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{Statistical Modeling}}\\
\hspace{1em}broom\textsuperscript{*} & Tidy model output; convert statistical objects into tidy tibbles & 2014 & Robinson \& Hayes & https://broom.tidymodels.org/\\
\hspace{1em}purrr & Functional programming tools for functions and vectors & 2015 & Henry \& Wickham & https://purrr.tidyverse.org/\\
\hspace{1em}caret & Framework for predictive modeling & 2007 & Kuhn & https://topepo.github.io/caret/index.html\\
\addlinespace[0.3em]
\multicolumn{5}{l}{\textbf{Data Visualization}}\\
\hspace{1em}ggplot2 & System for data visualization & 2007 & Wickham et al. & https://ggplot2.tidyverse.org/\\
\hspace{1em}kableExtra & Build complex tables and manipulate styles & 2017 & Zhu & https://haozhu233.github.io/kableExtra/\\
\hspace{1em}rmarkdown & Authoring framework for data science and reproducible research & 2014 & Allaire et al. & https://rmarkdown.rstudio.com/lesson-1.html\\
\bottomrule
\multicolumn{5}{l}{\textsuperscript{*} See the biobroom analog in Bioconductor}\\
\end{tabular}}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## trying to separate color; striped by group}
\CommentTok{#  row_spec(1:3 - 1, extra_latex_after = "\textbackslash{}\textbackslash{}rowcolor\{gray!6\}")}
\CommentTok{#  row_spec(0:3, extra_latex_after = "\textbackslash{}\textbackslash{}rowcolor\{orange!6\}") %>% }
\CommentTok{#  row_spec(4:6, extra_latex_after = "\textbackslash{}\textbackslash{}rowcolor\{gray!6\}") %>% }
\CommentTok{#  row_spec(7:11, extra_latex_after = "\textbackslash{}\textbackslash{}rowcolor\{gray!6\}")}

\CommentTok{## QUESTIONS }
\CommentTok{# Code font for package names in " "? \textbackslash{}texttt\{\}?}
\CommentTok{# How do you repeat same symbol on multiple items with one footnote?}
\CommentTok{# How do you separate colors and stripe by group?}
\CommentTok{# Add title}
\CommentTok{# Add caption}
\CommentTok{# Cite packages in bib and add references in table?}
\CommentTok{# Embed url link to package documentation? Do we want to link cheatsheets?}
\CommentTok{# How do you add link/reference to Table 1 in text in the template?}
\CommentTok{# How do you hide code for table in knitted pdf...include=FALSE errors?}
\CommentTok{# Title for column 2: description/purpose/usage?}
\CommentTok{# Length of description/purpose/usage for each package?}
\end{Highlighting}
\end{Shaded}

\hypertarget{supporting-information}{%
\section{Supporting information}\label{supporting-information}}

Do we need to include any supporting information?

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

{[}Acknowledgment of people who have helped; suggestions from
colleagues, etc.{]}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-perez2016}{}%
1. Perez-Riverol Y, Gatto L, Wang R, Sachsenberg T, Uszkoreit J, Veiga
Leprevost F da, et al. Ten simple rules for taking advantage of git and
github. PLoS computational biology. Public Library of Science; 2016;12.

\leavevmode\hypertarget{ref-peng2011}{}%
2. Peng RD. Reproducible research in computational science. Science.
American Association for the Advancement of Science; 2011;334:
1226--1227.

\leavevmode\hypertarget{ref-goodman2016}{}%
3. Goodman SN, Fanelli D, Ioannidis JP. What does research
reproducibility mean? Science translational medicine. American
Association for the Advancement of Science; 2016;8: 341ps12--341ps12.

\leavevmode\hypertarget{ref-donoho2017}{}%
4. Donoho D. 50 years of data science. Journal of Computational and
Graphical Statistics. Taylor \& Francis; 2017;26: 745--766.

\leavevmode\hypertarget{ref-stodden2013}{}%
5. Stodden V, Miguez S. Best practices for computational science:
Software infrastructure and environments for reproducible and extensible
research. Available at SSRN 2322276. 2013;

\leavevmode\hypertarget{ref-robinson2017}{}%
6. Robinson D. The impressive growth of r {[}Internet{]}. Stack
Overflow; 2017. Available:
\url{https://stackoverflow.blog/2017/10/10/impressive-growth-r/}

\leavevmode\hypertarget{ref-gentleman2004}{}%
7. Gentleman RC, Carey VJ, Bates DM, Bolstad B, Dettling M, Dudoit S, et
al. Bioconductor: Open software development for computational biology
and bioinformatics. Genome biology. Springer; 2004;5: R80.

\leavevmode\hypertarget{ref-holmes2018}{}%
8. Holmes S, Huber W. Modern statistics for modern biology
{[}Internet{]}. Cambridge University Press; 2018. Available:
\url{https://web.stanford.edu/class/bios221/book/index.html}

\leavevmode\hypertarget{ref-Rproject2020}{}%
9. Team RC. The r project for statistical computing {[}Internet{]}. The
R Foundation; 2020. Available: \url{https://www.r-project.org/}

\leavevmode\hypertarget{ref-hornik2012}{}%
10. Hornik K. Are there too many r packages? Austrian Journal of
Statistics. 2012;41: 59--66.

\leavevmode\hypertarget{ref-baseR}{}%
11. R Core Team. R: A language and environment for statistical computing
{[}Internet{]}. Vienna, Austria: R Foundation for Statistical Computing;
2019. Available: \url{https://www.R-project.org/}

\leavevmode\hypertarget{ref-dplyr}{}%
12. Wickham H, François R, Henry L, Müller K. Dplyr: A grammar of data
manipulation {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=dplyr}

\leavevmode\hypertarget{ref-tidyr}{}%
13. Wickham H, Henry L. Tidyr: Tidy messy data {[}Internet{]}. 2020.
Available: \url{https://CRAN.R-project.org/package=tidyr}

\leavevmode\hypertarget{ref-tidyverse}{}%
14. Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, et
al. Welcome to the tidyverse. Journal of Open Source Software. 2019;4:
1686.
doi:\href{https://doi.org/10.21105/joss.01686}{10.21105/joss.01686}

\leavevmode\hypertarget{ref-twitteR}{}%
15. Gentry J. TwitteR: R based twitter client {[}Internet{]}. 2015.
Available: \url{https://CRAN.R-project.org/package=twitteR}

\leavevmode\hypertarget{ref-mailR}{}%
16. Premraj R. MailR: A utility to send emails from r {[}Internet{]}.
2015. Available: \url{https://CRAN.R-project.org/package=mailR}

\leavevmode\hypertarget{ref-phonenumber}{}%
17. Myles S. Phonenumber: Convert letters to numbers and back as on a
telephone keypad {[}Internet{]}. 2015. Available:
\url{https://CRAN.R-project.org/package=phonenumber}

\leavevmode\hypertarget{ref-bioCproject}{}%
18. Huber W, Carey VJ, Gentleman R, Anders S, Carlson M, Carvalho BS, et
al. Orchestrating high-throughput genomic analysis with Bioconductor.
Nature Methods. 2015;12: 115--121. Available:
\url{http://www.nature.com/nmeth/journal/v12/n2/full/nmeth.3252.html}

\leavevmode\hypertarget{ref-ctv}{}%
19. Zeileis A. CRAN task views. R News. 2005;5: 39--40. Available:
\url{https://CRAN.R-project.org/doc/Rnews/}

\leavevmode\hypertarget{ref-zeileis2005}{}%
20. Zeileis A. CRAN task views. R News. 2005;5: 39--40.

\leavevmode\hypertarget{ref-smith2017}{}%
21. Smith D. The r community is one of r's best features {[}Internet{]}.
Revolutions. Microsoft; 2017. Available:
\url{https://blog.revolutionanalytics.com/2017/06/r-community.html}

\leavevmode\hypertarget{ref-ellis2017}{}%
22. Ellis SE. Hey! You there! You are welcome here {[}Internet{]}.
rOpenSci. NumFOCUS; 2017. Available:
\url{https://ropensci.org/blog/2017/06/23/community/}

\leavevmode\hypertarget{ref-rickert2018}{}%
23. Rickert J. What makes a great r package? {[}Internet{]}. RStudio;
2018. Available:
\url{https://rstudio.com/resources/rstudioconf-2018/what-makes-a-great-r-package-joseph-rickert/}

\leavevmode\hypertarget{ref-randomforest}{}%
24. Liaw A, Wiener M. Classification and regression by randomForest. R
News. 2002;2: 18--22. Available:
\url{https://CRAN.R-project.org/doc/Rnews/}

\leavevmode\hypertarget{ref-rayshader}{}%
25. Morgan-Wall T. Rayshader: Create maps and visualize data in 2D and
3D {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=rayshader}

\leavevmode\hypertarget{ref-broman2020}{}%
26. Broman K. Getting your r package on cran {[}Internet{]}. 2020.
Available: \url{https://kbroman.org/pkg_primer/pages/cran.html}

\leavevmode\hypertarget{ref-wickham2015}{}%
27. Wickham H. R packages: Organize, test, document, and share your
code. "O'Reilly Media, Inc."; 2015.

\leavevmode\hypertarget{ref-cranpolicy2020}{}%
28. CRAN repository policy {[}Internet{]}. The R Foundation; 2020.
Available:
\url{https://cran.r-project.org/web/packages/policies.html\#Submission}

\leavevmode\hypertarget{ref-weathermetrics}{}%
29. Anderson GB, Bell ML, Peng RD. Methods to calculate the heat index
as an exposure metric in environmental health research. Environemtental
Health Perspectives. 2013;121: 1111--1119. Available:
\url{http://ehp.niehs.nih.gov/1206273/}

\leavevmode\hypertarget{ref-bioconductor}{}%
30. Bioconductor: Open source software for bioinformatics
{[}Internet{]}. Bioconductor; 2020. Available:
\url{https://www.bioconductor.org/}

\leavevmode\hypertarget{ref-BiocManager}{}%
31. Morgan M. BiocManager: Access the bioconductor project package
repository {[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=BiocManager}

\leavevmode\hypertarget{ref-falcon2007}{}%
32. Falcon S, Morgan M, Gentleman R. An introduction to bioconductor's
expressionset class. 2007.

\leavevmode\hypertarget{ref-biocpkgsub2020}{}%
33. Package submission {[}Internet{]}. Bioconductor; 2020. Available:
\url{https://www.bioconductor.org/developers/package-submission/}

\leavevmode\hypertarget{ref-ropensci2020}{}%
34. Transforming science through open data and software {[}Internet{]}.
rOpenSci; 2020. Available: \url{https://ropensci.org/}

\leavevmode\hypertarget{ref-ropenscidev2020}{}%
35. rOpenSci, Anderson B, Chamberlain S, Krystalli A, Mullen L, Ram K,
et al. Ropensci/dev\_guide: Fourth release {[}Internet{]}. Zenodo; 2020.
doi:\href{https://doi.org/10.5281/zenodo.3749013}{10.5281/zenodo.3749013}

\leavevmode\hypertarget{ref-drat}{}%
36. Carl Boettiger DE with contributions by, Fultz N, Gibb S, Gillespie
C, Górecki J, Jones M, et al. Drat: 'Drat' r archive template
{[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=drat}

\leavevmode\hypertarget{ref-anderson2017}{}%
37. Anderson GB, Eddelbuettel D. Hosting data packages via drat: A case
study with hurricane exposure data. R Journal. 2017;9.

\leavevmode\hypertarget{ref-theussl2009}{}%
38. Theußl S, Zeileis A. Collaborative software development using
r-forge. Special invited paper on" the future of r". The R Journal. The
R Foundation for Statistical Computing; 2009;1: 9--14.

\leavevmode\hypertarget{ref-lang2000}{}%
39. Lang DT. The omegahat environment: New possibilities for statistical
computing. Journal of Computational and Graphical Statistics. Taylor \&
Francis; 2000;9: 423--451.

\leavevmode\hypertarget{ref-mcelreath2020}{}%
40. McElreath R. Statistical rethinking: A bayesian course with examples
in r and stan. CRC press; 2020.

\leavevmode\hypertarget{ref-decan2015}{}%
41. Decan A, Mens T, Claes M, Grosjean P. On the development and
distribution of r packages: An empirical analysis of the r ecosystem.
Proceedings of the 2015 european conference on software architecture
workshops. 2015. pp. 1--6.

\leavevmode\hypertarget{ref-devtools}{}%
42. Wickham H, Hester J, Chang W. Devtools: Tools to make developing r
packages easier {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=devtools}

\leavevmode\hypertarget{ref-decan2016}{}%
43. Decan A, Mens T, Claes M, Grosjean P. When github meets cran: An
analysis of inter-repository package dependency problems. 2016 ieee 23rd
international conference on software analysis, evolution, and
reengineering (saner). IEEE; 2016. pp. 493--504.

\leavevmode\hypertarget{ref-pkgdown}{}%
44. Wickham H, Hesselberth J. Pkgdown: Make static html documentation
for a package {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=pkgdown}

\leavevmode\hypertarget{ref-sf}{}%
45. Pebesma E. Simple Features for R: Standardized Support for Spatial
Vector Data. The R Journal. 2018;10: 439--446.
doi:\href{https://doi.org/10.32614/RJ-2018-009}{10.32614/RJ-2018-009}

\leavevmode\hypertarget{ref-learnr}{}%
46. Schloerke B, Allaire J, Borges B. Learnr: Interactive tutorials for
r {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=learnr}

\leavevmode\hypertarget{ref-ushey2020}{}%
47. Ushey K. RStudio 1.3 preview: Integrated tutorials {[}Internet{]}.
RStudio; 2020. Available:
\url{https://blog.rstudio.com/2020/02/25/rstudio-1-3-integrated-tutorials/}

\leavevmode\hypertarget{ref-pkgdownweb}{}%
48. Wickham H, Hesselberth J. Pkgdown {[}Internet{]}. RStudio; 2018.
Available: \url{https://pkgdown.r-lib.org/}

\leavevmode\hypertarget{ref-naniar}{}%
49. Tierney N, Cook D, McBain M, Fay C. Naniar: Data structures,
summaries, and visualisations for missing data {[}Internet{]}. 2020.
Available: \url{https://CRAN.R-project.org/package=naniar}

\leavevmode\hypertarget{ref-valr}{}%
50. Riemondy KA, Sheridan RM, Gillen A, Yu Y, Bennett CG, Hesselberth
JR. Valr: Reproducible genome interval arithmetic in r. F1000Research.
2017;
doi:\href{https://doi.org/10.12688/f1000research.11997.1}{10.12688/f1000research.11997.1}

\leavevmode\hypertarget{ref-TransPhylo}{}%
51. Didelot, Xavier, Fraser, Christophe, Gardy, Jennifer, et al. Genomic
infectious disease epidemiology in partially sampled and ongoing
outbreaks. Molecular Biology and Evolution. 2017;34: 997--1007.
doi:\href{https://doi.org/10.1093/molbev/msw275}{10.1093/molbev/msw275}

\leavevmode\hypertarget{ref-rdocumentation}{}%
52. RDocumentation {[}Internet{]}. DataCamp Inc. 2020. Available:
\url{https://www.rdocumentation.org/}

\leavevmode\hypertarget{ref-bookdown}{}%
53. Xie Y. Bookdown: Authoring books and technical documents with r
markdown {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=bookdown}

\leavevmode\hypertarget{ref-xie2016}{}%
54. Xie Y. Bookdown: Authoring books and technical documents with r
markdown. Chapman; Hall/CRC; 2016.

\leavevmode\hypertarget{ref-mgcv}{}%
55. Wood SN. Fast stable restricted maximum likelihood and marginal
likelihood estimation of semiparametric generalized linear models.
Journal of the Royal Statistical Society (B). 2011;73: 3--36.

\leavevmode\hypertarget{ref-tidymodels}{}%
56. Kuhn M, Wickham H. Tidymodels: A collection of packages for modeling
and machine learning using tidyverse principles. {[}Internet{]}. 2020.
Available: \url{https://www.tidymodels.org}

\leavevmode\hypertarget{ref-ggplot2}{}%
57. Wickham H. Ggplot2: Elegant graphics for data analysis
{[}Internet{]}. Springer-Verlag New York; 2016. Available:
\url{https://ggplot2.tidyverse.org}

\leavevmode\hypertarget{ref-shiny}{}%
58. Chang W, Cheng J, Allaire J, Xie Y, McPherson J. Shiny: Web
application framework for r {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=shiny}

\leavevmode\hypertarget{ref-htmlwidgets}{}%
59. Vaidyanathan R, Xie Y, Allaire J, Cheng J, Russell K. Htmlwidgets:
HTML widgets for r {[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=htmlwidgets}

\leavevmode\hypertarget{ref-Rcpp}{}%
60. Eddelbuettel D, François R. Rcpp: Seamless R and C++ integration.
Journal of Statistical Software. 2011;40: 1--18.
doi:\href{https://doi.org/10.18637/jss.v040.i08}{10.18637/jss.v040.i08}

\leavevmode\hypertarget{ref-caret}{}%
61. Kuhn M. Caret: Classification and regression training
{[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=caret}

\leavevmode\hypertarget{ref-datatable}{}%
62. Dowle M, Srinivasan A. Data.table: Extension of `data.frame`
{[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=data.table}

\leavevmode\hypertarget{ref-leaflet}{}%
63. Cheng J, Karambelkar B, Xie Y. Leaflet: Create interactive web maps
with the javascript 'leaflet' library {[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=leaflet}

\leavevmode\hypertarget{ref-randomizr}{}%
64. Coppock A. Randomizr: Easy-to-use tools for common forms of random
assignment and sampling {[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=randomizr}

\leavevmode\hypertarget{ref-stringr}{}%
65. Wickham H. Stringr: Simple, consistent wrappers for common string
operations {[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=stringr}

\leavevmode\hypertarget{ref-survminer}{}%
66. Kassambara A, Kosinski M, Biecek P. Survminer: Drawing survival
curves using 'ggplot2' {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=survminer}

\leavevmode\hypertarget{ref-Rmail2020}{}%
67. Team RC. Mailing lists {[}Internet{]}. The R Foundation; 2020.
Available: \url{https://www.r-project.org/mail.html}

\leavevmode\hypertarget{ref-chase2020}{}%
68. Chase W. Dataviz and the 20th anniversary of r, an interview with
hadley wickham {[}Internet{]}. Medium; 2020. Available:
\url{https://medium.com/nightingale/dataviz-and-the-20th-anniversary-of-r-an-interview-with-hadley-wickham-ea245078fc8a}

\leavevmode\hypertarget{ref-statnet}{}%
69. Handcock MS, Hunter DR, Butts CT, Goodreau SM, Krivitsky PN, Morris
M. Ergm: Fit, simulate and diagnose exponential-family models for
networks {[}Internet{]}. The Statnet Project
(\url{http://www.statnet.org}); 2018. Available:
\url{https://CRAN.R-project.org/package=ergm}

\leavevmode\hypertarget{ref-muenchen2012}{}%
70. Muenchen RA. The popularity of data analysis software. URL
http://r4statscom/popularity. 2012;

\leavevmode\hypertarget{ref-wickham2019}{}%
71. Wickham H. Advanced r. CRC press; 2019.

\leavevmode\hypertarget{ref-eddelbuettel2013}{}%
72. Eddelbuettel D. Seamless r and c++ integration with rcpp. Springer;
2013.

\leavevmode\hypertarget{ref-githubdocs}{}%
73. GitHub docs {[}Internet{]}. GitHub, Inc. 2020. Available:
\url{https://docs.github.com/en}

\leavevmode\hypertarget{ref-leek2015}{}%
74. Leek J. How i decide when to trust an r package {[}Internet{]}.
2015. Available:
\url{https://simplystatistics.org/2015/11/06/how-i-decide-when-to-trust-an-r-package/}

\leavevmode\hypertarget{ref-vannoorenberghe2017}{}%
75. Vannoorenberghe L. RDocumentation: Scoring and ranking
{[}Internet{]}. DataCamp; 2017. Available:
\url{https://www.datacamp.com/community/blog/rdocumentation-ranking-scoring}

\leavevmode\hypertarget{ref-packagemetrics}{}%
76. Firke S, Krouse B, Grand E, Shepherd L, Ampeh W, Frick H.
Packagemetrics: A package for helping you choose which package to use
{[}Internet{]}. 2017. Available:
\url{https://github.com/ropenscilabs/packagemetrics}

\leavevmode\hypertarget{ref-packagemetricsblog}{}%
77. Becca Krouse HF Erin Grand. Packagemetrics - helping you choose a
package since runconf17 {[}Internet{]}. rOpenSci; 2017. Available:
\url{https://ropensci.org/blog/2017/06/27/packagemetrics/}

\leavevmode\hypertarget{ref-smith2014}{}%
78. Smith A. Improving github for science {[}Internet{]}. GitHub, Inc.
2014. Available:
\url{https://github.blog/2014-05-14-improving-github-for-science/}

\leavevmode\hypertarget{ref-DESeq2}{}%
79. Love MI, Huber W, Anders S. Moderated estimation of fold change and
dispersion for rna-seq data with deseq2. Genome Biology. 2014;15: 550.
doi:\href{https://doi.org/10.1186/s13059-014-0550-8}{10.1186/s13059-014-0550-8}

\leavevmode\hypertarget{ref-flowCore}{}%
80. Ellis B, Haaland P, Hahne F, Le Meur N, Gopalakrishnan N, Spidlen J,
et al. FlowCore: FlowCore: Basic structures for flow cytometry data.
2019.

\leavevmode\hypertarget{ref-hahne2009}{}%
81. Hahne F, LeMeur N, Brinkman RR, Ellis B, Haaland P, Sarkar D, et al.
FlowCore: A bioconductor package for high throughput flow cytometry. BMC
bioinformatics. BioMed Central; 2009;10: 1--8.

\leavevmode\hypertarget{ref-taschuk2017}{}%
82. Taschuk M, Wilson G. Ten simple rules for making research software
more robust. PLoS computational biology. Public Library of Science;
2017;13.

\leavevmode\hypertarget{ref-hester2020}{}%
83. Hester J. How does covr work anyway? {[}Internet{]}. The R
Foundation; 2020. Available:
\url{https://cran.r-project.org/web/packages/covr/vignettes/how_it_works.html}

\leavevmode\hypertarget{ref-wickham2011}{}%
84. Wickham H. Testthat: Get started with testing. The R Journal.
2011;3: 5--10. Available:
\url{https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}

\leavevmode\hypertarget{ref-testthat}{}%
85. Wickham H. Testthat: Get started with testing. The R Journal.
2011;3: 5--10. Available:
\url{https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}

\leavevmode\hypertarget{ref-covr}{}%
86. Hester J. Covr: Test coverage for packages {[}Internet{]}. 2020.
Available: \url{https://CRAN.R-project.org/package=covr}

\leavevmode\hypertarget{ref-barts2018}{}%
87. Barts C. How to use github badges to stop feeling like a noob
{[}Internet{]}. freeCodeCamp; 2018. Available:
\url{https://www.freecodecamp.org/news/how-to-use-badges-to-stop-feeling-like-a-noob-d4e6600d37d2/}

\leavevmode\hypertarget{ref-wilson2006}{}%
88. Wilson GV. Where's the real bottleneck in scientific computing?
American Scientist. 2006;94: 5.

\leavevmode\hypertarget{ref-bryan2018}{}%
89. Bryan J. Excuse me, do you have a moment to talk about version
control? The American Statistician. Taylor \& Francis; 2018;72: 20--27.

\leavevmode\hypertarget{ref-perkel2016}{}%
90. Perkel J. Democratic databases: Science on github. Nature News.
2016;538: 127.

\leavevmode\hypertarget{ref-peng2017}{}%
91. Mastering software development in r {[}Internet{]}. 2017. Available:
\url{https://rdpeng.github.io/RProgDA/}

\leavevmode\hypertarget{ref-broom}{}%
92. Robinson D, Hayes A. Broom: Convert statistical analysis objects
into tidy tibbles {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=broom}

\leavevmode\hypertarget{ref-biobroom}{}%
93. Andrew J. Bass SL David G. Robinson. Biobroom: Turn bioconductor
objects into tidy data frames {[}Internet{]}. 2020. Available:
\url{https://github.com/StoreyLab/biobroom}

\leavevmode\hypertarget{ref-parker2013}{}%
94. Parker H. Personal r packages {[}Internet{]}. 2013. Available:
\url{https://hilaryparker.com/2013/04/03/personal-r-packages/}

\leavevmode\hypertarget{ref-Hmisc}{}%
95. Harrell Jr FE, Charles Dupont, others. Hmisc: Harrell miscellaneous
{[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=Hmisc}

\leavevmode\hypertarget{ref-broman}{}%
96. Broman KW. Broman: Karl broman's r code {[}Internet{]}. 2020.
Available: \url{https://CRAN.R-project.org/package=broman}

\leavevmode\hypertarget{ref-robinson2014}{}%
97. Robinson D. Broom: An r package for converting statistical analysis
objects into tidy data frames. arXiv preprint arXiv:14123565. 2014;

\leavevmode\hypertarget{ref-usethis}{}%
98. Wickham H, Bryan J. Usethis: Automate package and project setup
{[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=usethis}

\leavevmode\hypertarget{ref-roxygen2}{}%
99. Wickham H, Danenberg P, Csárdi G, Eugster M. Roxygen2: In-line
documentation for r {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=roxygen2}

\leavevmode\hypertarget{ref-rlang}{}%
100. Henry L, Wickham H. Rlang: Functions for base types and core r and
'tidyverse' features {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=rlang}

\leavevmode\hypertarget{ref-Rcore2020}{}%
101. Team RC. Writing r extensions {[}Internet{]}. The R Foundation;
2020. Available:
\url{https://cran.r-project.org/doc/manuals/R-exts.html}

\nolinenumbers


\end{document}

